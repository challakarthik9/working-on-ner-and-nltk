{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"HELLO I AM KARTHIK CHALLA\" #task 1 convert text to lower case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello i am karthik challa\n"
     ]
    }
   ],
   "source": [
    "print(lower_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HELLO', 'I', 'AM', 'KARTHIK', 'CHALLA']\n"
     ]
    }
   ],
   "source": [
    "#tokenize the sentences to get the tokens of the texti.e breaking the sentences into words\n",
    "text = \"HELLO I AM KARTHIK CHALLA\"\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' HELLO I AM KARTHIL CHALLA']\n"
     ]
    }
   ],
   "source": [
    "#sent tokenize\n",
    "#tokenize the sentences if the there are more than 1 sentence i.e breaking the sentences to list of sentences\n",
    "text = \" HELLO I AM KARTHIL CHALLA\"\n",
    "sent_token = nltk.sent_tokenize(text)\n",
    "print(sent_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HELLO', 'I', 'AM', 'KARTHIK', 'CHALLA']\n"
     ]
    }
   ],
   "source": [
    "#stop words removal - remove irrelevant words using nltk stop words like is, the , a etc from the sentences as they dont carry any information\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopword = stopwords.words('english')\n",
    "text = \"HELLO I AM KARTHIK CHALLA\"\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "removing_stopwords = [word for word in word_tokens if word not in stopword]\n",
    "print(removing_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HELLO', 'I', 'AM', 'KARTHIK', 'CHALLA']\n"
     ]
    }
   ],
   "source": [
    "#lemma - lemmatize the text to get its root form eg:funvtions, functionality as function\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stopword = stopwords.words('english')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "text = \"HELLO I AM KARTHIK CHALLA\"\n",
    "lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "print(lemmatized_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'i', 'am', 'karthik', 'challa']\n"
     ]
    }
   ],
   "source": [
    "#stemming\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "#is based on Porter Stemming Algorithm\n",
    "stopword = stopwords.words('english')\n",
    "snowball_stemmer = SnowballStemmer('english') #Snowball is a small string processing language designed for creating stemming algorithms for use in Information Retrieval\n",
    "text = \"HELLO I AM KARTHIK CHALLA\"\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "stemmed_word = [snowball_stemmer.stem(word) for word in word_tokens]\n",
    "print(stemmed_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ni', 1), ('andi', 1), ('nenu', 1), ('mee', 1), ('karthik', 1)]\n"
     ]
    }
   ],
   "source": [
    "#get word frequency = counting the word occurence using FreqDist library\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "text = \"HELLO ANDI NENU MEE KARTHIK CHALLA NI\"\n",
    "word = nltk.word_tokenize(text.lower())\n",
    "freq = FreqDist(word)\n",
    "print (freq.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('HELLO', 'NNP'), ('I', 'PRP'), ('AM', 'VBP'), ('KARTHIK', 'NNP'), ('CHALLA', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "#parts of speech tags \n",
    "import nltk\n",
    "text = \"HELLO I AM KARTHIK CHALLA\"\n",
    "word = nltk.word_tokenize(text)\n",
    "pos_tag = nltk.pos_tag(word)\n",
    "print (pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-15-da2476423bb9>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-da2476423bb9>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    2.\tCD\tCardinal number\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "###1.\tCC\tCoordinating conjunction\n",
    "\t2.\tCD\tCardinal number\n",
    "\t3.\tDT\tDeterminer\n",
    "\t4.\tEX\tExistential there\n",
    "\t5.\tFW\tForeign word\n",
    "\t6.\tIN\tPreposition or subordinating conjunction\n",
    "\t7.\tJJ\tAdjective\n",
    "\t8.\tJJR\tAdjective, comparative\n",
    "\t9.\tJJS\tAdjective, superlative\n",
    "\t10.\tLS\tList item marker\n",
    "\t11.\tMD\tModal\n",
    "\t12.\tNN\tNoun, singular or mass\n",
    "\t13.\tNNS\tNoun, plural\n",
    "\t14.\tNNP\tProper noun, singular\n",
    "\t15.\tNNPS\tProper noun, plural\n",
    "\t16.\tPDT\tPredeterminer\n",
    "\t17.\tPOS\tPossessive ending\n",
    "\t18.\tPRP\tPersonal pronoun\n",
    "\t19.\tPRP$\tPossessive pronoun\n",
    "\t20.\tRB\tAdverb\n",
    "\t21.\tRBR\tAdverb, comparative\n",
    "\t22.\tRBS\tAdverb, superlative\n",
    "\t23.\tRP\tParticle\n",
    "\t24.\tSYM\tSymbol\n",
    "\t25.\tTO\tto\n",
    "\t26.\tUH\tInterjection\n",
    "\t27.\tVB\tVerb, base form\n",
    "\t28.\tVBD\tVerb, past tense\n",
    "\t29.\tVBG\tVerb, gerund or present participle\n",
    "\t30.\tVBN\tVerb, past participle\n",
    "\t31.\tVBP\tVerb, non-3rd person singular present\n",
    "\t32.\tVBZ\tVerb, 3rd person singular present\n",
    "\t33.\tWDT\tWh-determiner\n",
    "\t34.\tWP\tWh-pronoun\n",
    "\t35.\tWP$\tPossessive wh-pronoun\n",
    "\t36.\tWRB\tWh-adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "text = \"who is Barrack Obama\"\n",
    "word = nltk.word_tokenize(text)\n",
    "pos_tag = nltk.pos_tag(word)\n",
    "chunk = nltk.ne_chunk(pos_tag)\n",
    "NE = [ \" \".join(w for w, t in ele) for ele in chunk if isinstance(ele, nltk.Tree)]\n",
    "print (NE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "article = '''\n",
    "Asian shares skidded on Tuesday after a rout in tech stocks put Wall Street to the sword, while a \n",
    "sharp drop in oil prices and political risks in Europe pushed the dollar to 16-month highs as investors dumped \n",
    "riskier assets. MSCI’s broadest index of Asia-Pacific shares outside Japan dropped 1.7 percent to a 1-1/2 \n",
    "week trough, with Australian shares sinking 1.6 percent. Japan’s Nikkei dived 3.1 percent led by losses in \n",
    "electric machinery makers and suppliers of Apple’s iphone parts. Sterling fell to $1.286 after three straight \n",
    "sessions of losses took it to the lowest since Nov.1 as there were still considerable unresolved issues with the\n",
    "European Union over Brexit, British Prime Minister Theresa May said on Monday.'''\n",
    "\n",
    "import nltk\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "print('NTLK Version: %s' % nltk.__version__)\n",
    "\n",
    "stanford_ner_tagger = StanfordNERTagger(\n",
    "    'stanford_ner/' + 'classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
    "    'stanford_ner/' + 'stanford-ner-3.9.2.jar'\n",
    ")\n",
    "\n",
    "results = stanford_ner_tagger.tag(article.split())\n",
    "\n",
    "print('Original Sentence: %s' % (article))\n",
    "for result in results:\n",
    "    tag_value = result[0]\n",
    "    tag_type = result[1]\n",
    "    if tag_type != 'O':\n",
    "        print('Type: %s, Value: %s' % (tag_type, tag_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunkSentenceTokenizer\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunkSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[5:]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
    "            namedEnt.draw()\n",
    "    except Exception as e:\n",
    "            print(str(e))\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "print(nltk.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.platform.startswith('win'):\n",
    "    # Common locations on Windows:\n",
    "    path += [\n",
    "        str(r'C:\\nltk_data'), str(r'D:\\nltk_data'), str(r'E:\\nltk_data'),\n",
    "        os.path.join(sys.prefix, str('nltk_data')),\n",
    "        os.path.join(sys.prefix, str('lib'), str('nltk_data')),\n",
    "        os.path.join(os.environ.get(str('APPDATA'), str('C:\\\\')), str('nltk_data'))\n",
    "    ]\n",
    "else:\n",
    "    # Common locations on UNIX & OS X:\n",
    "    path += [\n",
    "        str('/usr/share/nltk_data'),\n",
    "        str('/usr/local/share/nltk_data'),\n",
    "        str('/usr/lib/nltk_data'),\n",
    "        str('/usr/local/lib/nltk_data')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, PunktSentenceTokenizer\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# sample text\n",
    "sample = gutenberg.raw(\"bible-kjv.txt\")\n",
    "\n",
    "tok = sent_tokenize(sample)\n",
    "\n",
    "for x in range(5):\n",
    "    print(tok[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "            for category in movie_reviews.categories()\n",
    "            for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)\n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "word_features = list(all_words.keys())[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print((find_features(movie_reviews.words('filename.txt'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(find_features(rev), category) for (rev,category) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using naive bayes classifier\n",
    "training_set = featuresets[:1900]\n",
    "#set that we'll test against:\n",
    "testing_set =featuresets[1900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classifier accuracy percent:\", (nltk.classify.accuracy(classifer, testing_set)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-64923d7b2076>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow_most_inoformative_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "classifier.show_most_inoformative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-568eb8266b0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msave_classifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"naivebayes.pickle\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_classifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msave_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "save_classifier = open(\"naivebayes.pickle\", \"wb\")\n",
    "pickle.dump(classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\nNLTK was unable to find the \\\\Users\\\\Challa Karthik\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python35\\\\Scripts\\\\stanford-ner\\\\classifiers\\\\english.all.3class.distsim.crf.ser.gz file!\nUse software specific configuration paramaters or set the STANFORD_MODELS environment variable.\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-8c48b207a3a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStanfordNERTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\\\Users\\\\Challa Karthik\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python35\\\\Scripts\\\\stanford-ner\\\\classifiers\\\\english.all.3class.distsim.crf.ser.gz'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'\\\\Users\\\\Challa Karthik\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python35\\\\Scripts\\\\stanford-ner\\\\stanford-ner.jar'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" hello i am karthik challa\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mtokenized_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\challa karthik\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\nltk\\tag\\stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mStanfordNERTagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\challa karthik\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\nltk\\tag\\stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_filename, path_to_jar, encoding, verbose, java_options)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         self._stanford_model = find_file(\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[0mmodel_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'STANFORD_MODELS'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         )\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\challa karthik\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_file\u001b[1;34m(filename, env_vars, searchpath, file_names, url, verbose)\u001b[0m\n\u001b[0;32m    644\u001b[0m ):\n\u001b[0;32m    645\u001b[0m     return next(\n\u001b[1;32m--> 646\u001b[1;33m         \u001b[0mfind_file_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647\u001b[0m     )\n\u001b[0;32m    648\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\challa karthik\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[1;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[0;32m    637\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'\\n\\n  For more information on %s, see:\\n    <%s>'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'='\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the \\\\Users\\\\Challa Karthik\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python35\\\\Scripts\\\\stanford-ner\\\\classifiers\\\\english.all.3class.distsim.crf.ser.gz file!\nUse software specific configuration paramaters or set the STANFORD_MODELS environment variable.\n==========================================================================="
     ]
    }
   ],
   "source": [
    "#The parameters passed to the StanfordNERTagger class include:\n",
    "\n",
    "#Classification model path (3 class model used below)\n",
    "#Stanford tagger jar file path\n",
    "#Training data encoding (default of ASCII)\n",
    "\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "st = StanfordNERTagger(r'\\\\Users\\\\Challa Karthik\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python35\\\\Scripts\\\\stanford-ner\\\\classifiers\\\\english.all.3class.distsim.crf.ser.gz','\\\\Users\\\\Challa Karthik\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python35\\\\Scripts\\\\stanford-ner\\\\stanford-ner.jar')\n",
    "text = \" hello i am karthik challa\"\n",
    "tokenized_text = word_tokenize(text)\n",
    "classified_text = st.tag(tokenized_text)\n",
    "print(classified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "#The RegexpTokenizer class works by compiling your pattern, then calling re.findall() on your text.\n",
    "#You could do all this yourself using the re module, but RegexpTokenizer implements the TokenizerI interface,\n",
    "#just like all the word tokenizers from the previous recipe. This means it can be used by other parts of the NLTK package,\n",
    "#such as corpus readers, which we'll cover in detail in Chapter 3, Creating Custom Corpora. \n",
    "#Many corpus readers need a way to tokenize the text they're reading, and can take optional keyword arguments \n",
    "#specifying an instance of a TokenizerI subclass. \n",
    "#This way, you have the ability to provide your own tokenizer instance if the default tokenizer is unsuitable.\n",
    "\n",
    "#RegexpTokenizer can also work by matching the gaps, as opposed to the tokens. Instead of using re.findall(), \n",
    "#the RegexpTokenizer class will use re.split(). This is how the BlanklineTokenizer class in nltk.tokenize is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HELLO', 'I', 'AM', 'KARTHIK', 'CHALLA']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(\"[\\w]+\")\n",
    "tokenizer.tokenize(\"HELLO I AM KARTHIK CHALLA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HELLO', 'I', 'AM', 'KARTHIK', 'CHALLA']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "regexp_tokenize(\"HELLO I AM KARTHIK CHALLA.\", \"[\\w']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HELLO', 'I', 'AM', 'KARTHIK', 'CHALLA']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#simple white space tokenizer\n",
    "tokenizer = RegexpTokenizer('\\s+', gaps = True)\n",
    "tokenizer.tokenize(\"HELLO I AM KARTHIK CHALLA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
